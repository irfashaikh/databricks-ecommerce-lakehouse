Databricks Eâ€‘commerce Lakehouse Project
ğŸš€ Project Overview

This project demonstrates an endâ€‘toâ€‘end Data Engineering pipeline on Databricks using an Eâ€‘commerce dataset from Kaggle, designed and implemented following Lakehouse + Medallion Architecture (Bronze, Silver, Gold) best practices.
The goal of this project is to showcase realâ€‘world Databricks developer skills including incremental ingestion, schema management, Delta Lake operations, and productionâ€‘ready project structuring with GitHub version control.


ğŸ—ï¸ Architecture
Medallion Architecture (Lakehouse Pattern)

Kaggle Dataset
      â”‚
      â–¼
 Bronze Layer  â†’  Silver Layer  â†’  Gold Layer
 (Raw Data)      (Cleaned)        (Business Metrics)

Each layer is implemented as Delta tables with clear responsibilities and isolation.


ğŸ”§ Technology Stack
Databricks
Apache Spark (PySpark)
Delta Lake
Databricks Auto Loader
Databricks Workflows (Jobs)
GitHub (Version Control)
Kaggle Dataset (Eâ€‘commerce)


ğŸ“‚ Repository Structure
databricks-ecommerce-lakehouse/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ bronze/
â”‚   â”œâ”€â”€ silver/
â”‚   â””â”€â”€ gold/
â”‚
â”œâ”€â”€ schemas/
â”‚   â”œâ”€â”€ bronze_ecommerce_schema.json
â”‚   â”œâ”€â”€ silver_ecommerce_schema.json
â”‚   â””â”€â”€ gold_ecommerce_metrics_schema.json
â”‚
â”œâ”€â”€ jobs/
â”‚   â””â”€â”€ databricks_job_config.json
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ common_functions.py
â”‚   â””â”€â”€ constants.py
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw_sample/
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ medallion_architecture.md
â”‚   â””â”€â”€ bronze_data_flow.png
â”‚
â””â”€â”€ README.md


ğŸ¥‰ Bronze Layer â€“ Raw Ingestion
Purpose: Capture raw data exactly as received.

Key Features :- 
    Incremental ingestion using Databricks Auto Loader (cloudFiles)
    Schema enforcement

Metadata columns added :-
    ingestion_time
    source_file
    load_date

Stored as Delta tables :-
    Why this matters
    This ensures data traceability, replay capability, and fault tolerance.


ğŸ¥ˆ Silver Layer â€“ Clean & Transform
Purpose: Prepare analyticsâ€‘ready data.

Operations Performe :-
    Data cleansing & standardization
    Handling nulls and invalid records
    Deduplication
    Incremental processing using Delta MERGE INTO (SCD Type 1)
    Business rule validations

Why MERGE is used:-
    Efficient upserts
    Handles lateâ€‘arriving data
    Idempotent processing


ğŸ¥‡ Gold Layer â€“ Business Aggregates
Purpose: Serve business and analytics use cases.

Outputs :-
    Revenue metrics
    Daily / monthly sales
    Top products
    Customerâ€‘level KPIs

Characteristics :-
    Optimized for BI & reporting
    Aggregated Delta tables
    Starâ€‘schema friendly design


ğŸ“ Schema Management (Productionâ€‘Grade)
All table schemas are autoâ€‘extracted directly from Delta tables
Stored as JSON (Spark StructType) under /schemas
Enables:
    Schema version control
    Change tracking
    Reusability & validation
Schemas are not hardcoded and are generated using a dedicated Databricks schemaâ€‘export notebook.


âš™ï¸ Job Orchestration
Implemented using Databricks Workflows (Jobs)

Multiâ€‘task job structure:
    Bronze ingestion
    Silver transformations
    Gold aggregations
Retry logic and monitoring enabled
Job configuration is exported as JSON for reproducibility.


ğŸ”„ Incremental Data Processing
Bronze â†’ Silver â†’ Gold pipelines are fully incremental
Designed to handle:
    New data arrivals
    Lateâ€‘arriving records
    Reâ€‘processing without duplication


ğŸ“Š Sample Business Metrics
Total Revenue
Daily Order Count
Average Order Value
Top Selling Products


ğŸš€ How to Run This Project
Upload sample Kaggle data to DBFS / cloud storage
Execute Bronze layer notebooks
Run Silver layer transformation notebooks
Build Gold layer aggregates
Trigger Databricks Job for endâ€‘toâ€‘end execution


ğŸ”’ Data & Security Notes
Full Kaggle dataset is not committed to GitHub
Only sample data is included
No secrets or credentials are stored in the repository


ğŸ¯ Key Learnings & Highlights
Realâ€‘world Databricks Lakehouse implementation
Strong understanding of Medallion Architecture
Delta Lake MERGE & incremental design
Schema governance and version control
Productionâ€‘ready GitHub project structure


ğŸ‘¤ Author
Irfan Shaikh
Data Engineer | Databricks | PySpark | Delta Lake

ğŸ“Œ Future Enhancements
Data quality checks with expectations
Schema drift detection
CI/CD integration for Databricks jobs
BI dashboard integration
